---
title: "Group 2 work file"
author: "Yu, Eunice, Aditya"
date: "2023-03-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r import data, echo=FALSE, warning=FALSE}
library(knitr)
cwd <- dirname(rstudioapi::getSourceEditorContext()$path)
competition_data <- read.csv(paste(cwd, "competition-data.csv", sep = "/"))
head(competition_data)
```

```{r na value}
na_values <- sum(is.na(competition_data))
na_values
# Here the data has no N.A. value
```
# select predictors
```{r}
library(corrplot)
cor_matrix <- cor(competition_data)
corrplot(cor_matrix, type = "upper", order = "hclust")
```
```{r cor plot}
corr_matrix <- cor(competition_data)
sorted_corr_matrix <- sort(corr_matrix["outcome",], decreasing = TRUE)
top_10_variables <- names(sorted_corr_matrix)[2:11]
top_10_variables

# Here these are the 10 variables that are correlated to the outcome variable
# But this code only gives us the predictors that are positively co related, thus we decided not to go ahead with these predictors.
```
```{r library}
library(ggplot2)
library(dplyr)
library(tidyverse)

pairs(competition_data %>% dplyr::select(X1:X10, outcome))

pairs(competition_data %>% dplyr::select(X11:X21, outcome))

pairs(competition_data%>% dplyr::select(outcome, X2, X3, X4, X5, X9, X11, X15, X18, X19, X20, X21))
```

```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(caret)
predictors_only<- dplyr::select(competition_data,X2, X3, X4, X5, X9, X11, X15, X18, X19, X20, X21)
dplyr:::select.data.frame(competition_data, outcome)
df_predictors_only <- data.frame(predictors_only)
Preprocessing_fit <- preProcess(
df_predictors_only,
method = c("BoxCox", "center", "scale"))
Preprocessing_fit
transformed_predictors <- predict(
Preprocessing_fit, df_predictors_only)
Processed_data<-cbind(transformed_predictors,competition_data$outcome)
colnames(Processed_data)[12] <- "outcome"
```

```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(earth)
library(Formula)
library(plotmo)
library(plotrix)
library(TeachingDemos)
set.seed(123)
ctrl = trainControl(
  method="cv", number = 10
)
```

#Linear Regression Model.
```{r}
set.seed(123)

lmTune <- train(outcome ~ ., data = Processed_data ,
                   method = "lm",
                   trControl = ctrl)
lmTune$results$RMSE
```

# Ridge Regression Model.
```{r}
set.seed(100)
ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 10))
ridgeTune <- train(outcome ~ ., data = Processed_data ,
                     method = "ridge",
                     tuneGrid = ridgeGrid,
                     trControl = ctrl,
                     preProc = c("center", "scale")
                )
ridgeTune$results$RMSE
```

```{r}
plot(ridgeTune)
ridgeTune$bestTune
min(ridgeTune$results$RMSE)
```

#LASSO.
```{r}
set.seed(100)
  lassoGrid <- expand.grid(lambda = c(0), 
                        fraction = seq(.1, 1, length = 15))
  lassoTune <- train(outcome ~ ., data = Processed_data ,
                      method = "enet",
                      tuneGrid = lassoGrid,
                      trControl = ctrl,
                      preProc = c("center", "scale")
                    )
  lassoTune$results$RMSE
```

```{r}
plot(lassoTune)
lassoTune$bestTune
min(lassoTune$results$RMSE)
```
### - Multivariate Adaptive Regression Splines (MARS)  
```{r,echo=FALSE,warning=FALSE,message=FALSE}
set.seed(102)
marsGrid <- expand.grid(.degree = 1:2, .nprune = c(20, 30, 40, 50))
    marsFit <- train(outcome ~ ., data = Processed_data ,
                       method = "earth",
                       tuneGrid = marsGrid,
                       trControl = ctrl
    )
    marsFit
```  
### - Random forests
```{r,echo=FALSE,warning=FALSE,message=FALSE}
set.seed(102)
mtryGrid <- data.frame(
  mtry = floor(seq(1, ncol(Processed_data), 
                      length = 2))
  )
  rfTune <- train(outcome ~ ., data = Processed_data ,
                   method = "rf",
                   tuneGrid = mtryGrid,
                   ntree = 50,
                   importance = TRUE,
                   trControl = ctrl)
    rfTune
```
```{r}
predicted_outcome <- predict(rfTune,transformed_test_predictors )
head(predicted_outcome)
View(predicted_outcome)
write.csv(predicted_outcome, file = "predicted_outcome.csv")
```


## KNN
```{r non-linear - K-nearest neighnours, echo = FALSE, warning = FALSE, message = FALSE}

ctrl = trainControl(
  method="cv", number = 10,
  selectionFunction = "oneSE" 
)
library(tidyverse)


knnFit <- train(outcome ~ ., data = Processed_data ,
                method="knn", 
                trControl=ctrl,
                tuneLength=15
          )

knnFit
```

```{r}
library(kernlab)
library(caret)

detach("package:kernlab", unload=TRUE)

  svmLinearTuned <- train(outcome ~ ., data = Processed_data ,
    method = "svmLinear",
    preProc = c("center", "scale"),
    tuneLength = 8,
    epsilon = 0.01,
    trControl = ctrl
  )
  
svmLinearTuned  
```

```{r}
library(knitr)
cwd <- dirname(rstudioapi::getSourceEditorContext()$path)
competition_test_data <- read.csv(paste(cwd, "competition-test-x-values.csv", sep = "/"))
head(competition_test_data)
```
```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(caret)
test_predictors_only<- dplyr::select(competition_test_data,X2, X3, X4, X5, X9, X11, X15, X18, X19, X20, X21)
#dplyr:::select.data.frame(competition_test_data, outcome)
df_test_predictors_only <- data.frame(test_predictors_only)
Preprocessing_test_fit <- preProcess(
df_test_predictors_only,
method = c("BoxCox", "center", "scale"))
Preprocessing_test_fit
transformed_test_predictors <- predict(
Preprocessing_test_fit, df_test_predictors_only)
#Processed_test_data<-cbind(transformed_test_predictors,competition_test_data$outcome)
#colnames(Processed_test_data)[12] <- "outcome"
```

